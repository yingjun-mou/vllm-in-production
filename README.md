# vllm-in-production
A tiny production-shaped LLM service that uses vLLM as the model server (OpenAI-compatible), wrapped by a gateway that adds auth, rate-limits, and metrics, plus a simple benchmarking script and dashboards.
