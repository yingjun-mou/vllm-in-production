name: vllm-gcp-stack

services:
  vllm:
    image: vllm/vllm-openai:v0.9.2
    command: >
      --model ${MODEL_ID}
      --dtype auto
      --host 0.0.0.0
      --port 8000
      --api-key ${API_KEY}
      --max-model-len ${MAX_MODEL_LEN:-32768}
    environment:
      - HF_TOKEN=${HF_TOKEN}
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    ports:
      - "8000:8000"
    volumes:
      - hf-cache:/root/.cache/huggingface
    networks: [monitoring]

  prometheus:
    image: prom/prometheus:v2.54.0
    command: ["--config.file=/etc/prometheus/prometheus.yml"]
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml:ro
    ports:
      - "9090:9090"
    networks: [monitoring]

  grafana:
    image: grafana/grafana:10.4.6
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
    volumes:
      - ./grafana/provisioning:/etc/grafana/provisioning
      - grafana-data:/var/lib/grafana
    ports:
      - "3000:3000"
    depends_on: [prometheus]
    networks: [monitoring]

  dcgm-exporter:
    image: nvidia/dcgm-exporter:latest
    runtime: nvidia
    pid: "host"
    ports:
      - "9400:9400"
    networks: [monitoring]

  node-exporter:
    image: prom/node-exporter:v1.8.2
    pid: host
    # Keep host mounts; no need for host network because Prometheus scrapes other services
    command:
      - '--path.rootfs=/host'
    volumes:
      - '/:/host:ro,rslave'
    networks: [monitoring]

  cadvisor:
    image: gcr.io/cadvisor/cadvisor:v0.49.1
    privileged: true
    # no host port needed; Prometheus scrapes via service name
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
    networks: [monitoring]

volumes:
  hf-cache:
  grafana-data:

networks:
  monitoring:
